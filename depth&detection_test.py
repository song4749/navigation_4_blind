from ultralytics import YOLO
import cv2
import numpy as np
import onnxruntime

# ğŸ”¹ YOLO ONNX ëª¨ë¸ ë¡œë“œ
model_1 = YOLO("object_detection/block_best.onnx", task="detect")  # ì ì ë¸”ë¡ íƒì§€ ëª¨ë¸
model_2 = YOLO("object_detection/obstacle_test.onnx", task="detect")  # ì¥ì• ë¬¼ íƒì§€ ëª¨ë¸

# ğŸ”¹ ê¹Šì´ ì¸¡ì • ONNX ëª¨ë¸ ë¡œë“œ
depth_model = onnxruntime.InferenceSession("object_depth/midas_small.onnx")

def preprocess_depth_image(image, size=(128, 128)):
    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0
    img = cv2.resize(img, size)
    img = img.transpose(2, 0, 1).astype(np.float32)
    img = np.expand_dims(img, axis=0)
    return img

def estimate_depth(image):
    input_tensor = preprocess_depth_image(image)
    input_name = depth_model.get_inputs()[0].name
    depth_map = depth_model.run(None, {input_name: input_tensor})[0]
    depth_map = np.squeeze(depth_map)
    depth_map = cv2.resize(depth_map, (image.shape[1], image.shape[0]))
    return depth_map

# ğŸ”¹ ë™ì˜ìƒ íŒŒì¼ ì—´ê¸°
video_path = "KakaoTalk_20250320_144019708.mp4"  # ì‚¬ìš©í•  ë™ì˜ìƒ ê²½ë¡œ
cap = cv2.VideoCapture(video_path)

# ğŸ”¹ ë¹„ë””ì˜¤ ì €ì¥ ì„¤ì •
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter("output_with_alerts.mp4", fourcc, cap.get(cv2.CAP_PROP_FPS), (int(cap.get(3)), int(cap.get(4))))

# ğŸ”¹ ì¶œë ¥ ì°½ í¬ê¸° ì„¤ì • (ë¹„ìœ¨ ìœ ì§€)
display_scale = 0.5  # 50% í¬ê¸°ë¡œ ì¶•ì†Œ

frame_count = 0
frame_total = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_count += 1
    print(f"[INFO] Processing frame {frame_count} / {frame_total}")

    results_1 = list(model_1.predict(frame, imgsz=640, conf=0.5, stream=True))
    results_2 = list(model_2.predict(frame, imgsz=640, conf=0.5, stream=True))

    # ğŸ”¹ ê¹Šì´ë§µ ì¶”ì • (í™”ë©´ì—ëŠ” í‘œì‹œí•˜ì§€ ì•ŠìŒ)
    depth_map = estimate_depth(frame)

    # ğŸ”¹ ê¹Šì´ ì»¬ëŸ¬ë§µ ìƒì„± (ì£¼ì„ì²˜ë¦¬: ë‚˜ì¤‘ì— ì‹œê°í™”í•˜ê³  ì‹¶ì„ ë•Œ ì‚¬ìš©)
    # depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_map, alpha=255.0 / depth_map.max()), cv2.COLORMAP_INFERNO)

    for r in results_1 + results_2:
        for box in r.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = box.conf[0]
            cls_id = int(box.cls[0])
            class_name = r.names[cls_id] if cls_id < len(r.names) else "Unknown"

            color = (0, 255, 0) if r in results_1 else (0, 0, 255)
            label_text = f"{class_name} {conf:.2f}"

            # ğŸ”¹ ê¹Šì´ ê¸°ë°˜ ê²½ê³  íŒë‹¨ (ê³„ì† í™œì„±í™”)
            if r in results_2:
                cx = int((x1 + x2) / 2)
                cy = int((y1 + y2) / 2)
                depth_value = depth_map[cy, cx]
                # print(f"[DEBUG] {class_name} depth value at ({cx},{cy}): {depth_value:.3f}")
                if depth_value > 500:
                    warning_text = "Danger"
                    color = (0, 0, 255)
                elif depth_value < 100:
                    warning_text = "Warning"
                    color = (0, 165, 255)
                else:
                    warning_text = "Safe"
                    color = (0, 255, 0)
                label_text += f" | {warning_text}"

            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
            cv2.putText(frame, label_text, (x1, y1 - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)

    # ğŸ”¹ ê²°ê³¼ í”„ë ˆì„ ì €ì¥
    out.write(frame)

    # ğŸ”¹ ë¹„ìœ¨ ìœ ì§€í•˜ë©´ì„œ í™”ë©´ ì¶•ì†Œí•´ì„œ ì¶œë ¥ (ì£¼ì„ì²˜ë¦¬: ë‚˜ì¤‘ì— ì‹¤ì‹œê°„ìœ¼ë¡œ ë³´ê³  ì‹¶ì„ ë•Œ í•´ì œ)
    # resized = cv2.resize(frame, (0, 0), fx=display_scale, fy=display_scale)
    # cv2.imshow("YOLO Object Detection (Video)", resized)

    # if cv2.waitKey(1) & 0xFF == 27:
    #     break

cap.release()
out.release()
cv2.destroyAllWindows()
