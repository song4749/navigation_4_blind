import cv2
import numpy as np
import onnxruntime
from ultralytics import YOLO

# ğŸ”¹ ONNX ëª¨ë¸ ë¡œë“œ
depth_model = onnxruntime.InferenceSession("object_depth/midas_small.onnx")  # ê¹Šì´ ì¸¡ì • ëª¨ë¸
yolo_model = YOLO("object_detection/best.onnx")  # ì¥ì• ë¬¼ íƒì§€ ëª¨ë¸

# ğŸ”¹ ì›¹ìº  ì„¤ì •
cap = cv2.VideoCapture(0)  # ê¸°ë³¸ ì›¹ìº 
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# ğŸ”¹ ì¥ì• ë¬¼ ê°ì§€ ê±°ë¦¬ ì„ê³„ê°’ (ì´ ê°’ ì´í•˜ì´ë©´ ê²½ê³ )
depth_threshold = 100

# ğŸ”¹ í´ë˜ìŠ¤ ëª©ë¡ (ì˜ˆì‹œ, data.yaml ì°¸ê³ )
classes = ['barricade', 'bench', 'bicycle', 'bollard', 'bus', 'car', 'carrier', 'chair', 'dog', 'fire_hydrant', 'kiosk', 'motorcycle', 'movable_signage', 'parking_meter', 'person', 'pole', 'potted_plant', 'scooter', 'stop', 'stroller', 'table', 'traffic_light', 'traffic_sign', 'tree_trunk', 'truck', 'wheelchair']

def preprocess_image(image, size=(128, 128)):
    """ MiDaS ONNX ëª¨ë¸ ì…ë ¥ì„ ìœ„í•œ ì´ë¯¸ì§€ ì „ì²˜ë¦¬ """
    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) / 255.0  # ì •ê·œí™”
    img = cv2.resize(img, size)
    img = img.transpose(2, 0, 1).astype(np.float32)  # (H, W, C) â†’ (C, H, W)
    img = np.expand_dims(img, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€
    return img

def estimate_depth(image):
    """ ê¹Šì´ ì¸¡ì • ì‹¤í–‰ """
    input_tensor = preprocess_image(image)
    input_name = depth_model.get_inputs()[0].name
    depth_map = depth_model.run(None, {input_name: input_tensor})[0]
    depth_map = np.squeeze(depth_map)  # (1, H, W) â†’ (H, W)
    depth_map = cv2.resize(depth_map, (image.shape[1], image.shape[0]))  # ì›ë³¸ í¬ê¸°ë¡œ ë³€í™˜
    return depth_map

# ğŸ”¹ ì‹¤ì‹œê°„ ì‹¤í–‰
while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # 1ï¸âƒ£ YOLO ì¥ì• ë¬¼ ê°ì§€ ì‹¤í–‰
    results = yolo_model.predict(frame)

    # 2ï¸âƒ£ MiDaS ê¹Šì´ ì¸¡ì • ì‹¤í–‰
    depth_map = estimate_depth(frame)

    # 3ï¸âƒ£ ê¹Šì´ ë§µì„ ì»¬ëŸ¬ë§µìœ¼ë¡œ ë³€í™˜
    depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_map, alpha=255.0 / depth_map.max()), cv2.COLORMAP_INFERNO)

    # 4ï¸âƒ£ ì¥ì• ë¬¼ ê°ì§€ëœ ê°ì²´ í‘œì‹œ
    for result in results:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = box.conf[0]
            cls_id = int(box.cls[0])

            if conf > 0.5:  # ì‹ ë¢°ë„ í•„í„°ë§
                label = classes[cls_id] if cls_id < len(classes) else "Unknown"
                color = (0, 255, 0) if label == "person" else (0, 0, 255)
                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)
                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

                # 5ï¸âƒ£ ì¥ì• ë¬¼ ê±°ë¦¬ ê²½ê³ 
                obstacle_region = depth_map[y1:y2, x1:x2]
                min_depth = np.min(obstacle_region)  # í•´ë‹¹ ì˜ì—­ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ê¹Šì´ê°’

                if min_depth < depth_threshold:
                    cv2.putText(frame, "âš ï¸ WARNING: Obstacle Close!", (50, 100),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), 3, cv2.LINE_AA)

    # 6ï¸âƒ£ ì›ë³¸ ì˜ìƒê³¼ ê¹Šì´ ë§µì„ ë‚˜ë€íˆ ë°°ì¹˜í•˜ì—¬ í‘œì‹œ
    new_size = (frame.shape[1] // 2, frame.shape[0] // 2)
    frame_resized = cv2.resize(frame, new_size)
    depth_colormap_resized = cv2.resize(depth_colormap, new_size)
    combined_output = cv2.hconcat([frame_resized, depth_colormap_resized])

    cv2.imshow("Obstacle Detection & Depth Map", combined_output)

    # ESC í‚¤ ì…ë ¥ ì‹œ ì¢…ë£Œ
    if cv2.waitKey(1) & 0xFF == 27:
        break

cap.release()
cv2.destroyAllWindows()
