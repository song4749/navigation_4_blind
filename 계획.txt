1. 처음 감지된 장애물의 바운딩 박스 크기 와 시간이 조금 지나 바운딩 박스가 커지면 경고를 주기
2. 처음 감지된 장애물의 세그맨테이션의 넓이와 시간이 지나고 넓이가 커지면 경고를 주기
3. 일정 거리마다의 장애물을 학습시키고 근접거리에서의 장애물이 탑지되면 경고주기
4. MiDaS, LiDAR


MiDaS 좋네 이걸로 가자
1. 장애물을 yolo 객체인식함 (세그맨테이션은 더 정확하지만 성능이 어떨지.. 실시간이 중요함) 테스트 정도만? 
2. 바운딩 박스 혹은 세그맨테이션의 픽셀 부분의 깊이를 알아냄
3. 이 부분의 깊이가 일정거리 이하로 가까워지면 경고를 출력
4. MiDaS는 절대적인 거리(미터 단위의 깊이)를 출력하는 것이 아니라, 상대적인 깊이 값을 예측한다.
yolo 버전마다 테스트 해보자


모바일 앱은 두가지 방법
1. 모바일에서 직접 실행 or 서버에서 실행
2. 모바일에서의 실행은 두가지 방법
2-1. MiDaS 모델, yolo 모델을 onnx로 변환해서 이걸 또 TensorFlow Lite(TFLite)로 변환하고 실행
2-2. Kivy 또는 PySide로 Python 기반 모바일 앱 개발(근데 이건 성능이 낮다고 함)
3. 서버에서 실행은 네트워크 연결이 필요. 카메라 화면만 서버로 전송하고 서버에서 딥러닝 실행
서버다 보니까 성능이 매우 좋음. 다만 실시간은 어떨지..


https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=189
데이터셋은 빈도수 높은거만 뽑고 용량 좀 줄여야함 6개 정도
눈으로 확인해보는게 중요함

서버 여는법
# 터미널 1 (FastAPI)
uvicorn fast_api.app:app --host 0.0.0.0 --port 8000

# 터미널 2 (Tunnel)
cloudflared tunnel run blindnavi-tunnel


project copy 를 복사해서 진행

fast_api => detection_router
navigation_app => navigation_router 두가지로 분할하고
combined_app.py로 합침 


navigation_4_blind/
├── fast_api/
│   ├── detection_router.py       👈 탐지 기능 담당
│   ├── models/                   👈 ONNX 등 모델
│   ├── utils/                    👈 유틸 함수
│   └── mp3/                      👈 TTS 캐시 등 (선택)
│
├── navigation_app/
│   ├── navigation_router.py      👈 경로 안내/검색 담당
│   ├── static/                   👈 JS, CSS, 이미지
│   └── templates/                👈 HTML 템플릿
│       ├── index.html            ← 네비게이션 페이지
│       └── detection.html        ← 실시간 탐지 페이지 ← ✅ 꼭 필요
│
├── combined_app.py               👈 FastAPI 통합 앱 실행 파일


현재까지의 파일 구조 

현재까지 서버가 열리긴하는데....
/navigation
/detection 이런식으로 페이지 구분이 안됩니다.
 